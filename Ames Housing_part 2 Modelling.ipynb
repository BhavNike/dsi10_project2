{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "...... continued from previous Jupyter Notebook 1 project 2\n",
    "\n",
    "### Contents:\n",
    "- [IMPORTING OF RELEVANT LIBRARIES](#IMPORTING_OF_RELEVANT_LIBRARIES)\n",
    "- [5. CLEANING DATASET](continued]\n",
    "    - [F. GETTING THE DATA FROM CSV FORMAT AND MERGING TO GET_DUMMIES](#GETTING_THE_DATA_FROM_CSV_FORMAT )\n",
    "    - [G. USING 'GET_DUMMIES' FUNCTION TO CONVERT NOMINAL DATA TO BINARY OUTPUT](#USING_'GET_DUMMIES'_FUNCTION)\n",
    "    - [H. SAVING FINAL DATASETS IN CSV FORMAT](#SAVING_FINAL_'TRAIN'_DATASET_DATAFRAME_IN_CSV_FORMAT)\n",
    "- [6. IDENTIFYING FEATURES FOR DESIGNING THE MODEL](#IDENTIFYING_FEATURES_FOR_DESIGNING_THE_MODEL)\n",
    "    - [1. Filtering by correlation between the features and the target variable](#Filtering_by_correlation)\n",
    "    - [2. Recursive Feature Engineering (RFE) to identify optimum number of features](#Recursive_Feature_Engineering)  \n",
    "- [7. MODEL BUILDING](#MODEL_BUILDING)\n",
    "    - [A. MODEL PREPARATION](#MODEL_PREPARATION)\n",
    "    - [B. Model 1 - correlated_features 1](#Model_1_-_corr_features1)\n",
    "    - [C. Model 2 - correlated features 2](#Model_2_-_corr_features2)\n",
    "    - [D. Model 3 - rfe features](#Model_3_-_rfe_features)\n",
    "- [8. EVALUATION OF THE MODELS](#EVALUATION_OF_THE_MODELS)\n",
    "- [9. SUBMISSION TO KAGGLE](#SUBMISSION_TO_KAGGLE)\n",
    "- [10. CONCLUSION](#CONCLUSION)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING OF RELEVANT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting options to view entire dataset\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. GETTING THE DATA FROM CSV FORMAT - 'df_train_alt' and 'df_test_alt'\n",
    "\n",
    "#### In the previous Jupyter notebook, 'train' dataset was cleaned for missing and null values and ordinal values replaced with integers. The final 'train' dataset was saved as csv file 'df_train_alt_aft_ord.csv'. Similarly the 'test' dataset was also cleaned and ordinal values replaced with integers. The final 'test' dataset was saved as 'df_test_alt_aft_ord.csv'. \n",
    "\n",
    "#### In this Jupyter notebook, the 2 dataframes will be merged and the dummy columns included for nominal values.\n",
    "\n",
    "#### The model design will start with selection of features through RFE method and Filtering methods and use of Linear Regression and the regularisation of Lasso and Ridge to study which gives the best fit.\n",
    "\n",
    "#### The final predictions of the selected model are uploaded to Kaggle to get the Kaggle score. \n",
    "\n",
    "#### The Kaggle score can be used as one of the indicators of robustness of the model. This is in addition to other measures of robustness such as evaluation of R square score and the Root Square Mean of Errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data from csv files - training data ('df_train_alt_aft_ord.csv')\n",
    "\n",
    "df_train_alt = pd.read_csv('../datasets/df_train_alt_aft_ord.csv')\n",
    "df_train_alt.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df_train_alt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting data from csv files - test data (df_test_alt_aft_ord.csv)\n",
    "\n",
    "df_test_alt = pd.read_csv('../datasets/df_test_alt_aft_ord.csv') \n",
    "df_test_alt.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df_test_alt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_alt.shape, df_test_alt.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. MERGING TRAIN AND TEST DATASETS TO PROCEED TO GET_DUMMIES FOR NOMINAL  DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging train and test data \n",
    "\n",
    "df_merged = pd.concat([df_train_alt, df_test_alt], sort=False)\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## creating dummy columns for nominal columns - df_merged_dummies\n",
    "\n",
    "columns_to_getdummies =['MS SubClass','MS Zoning', 'Alley', 'Land Contour', 'Lot Config', 'Neighborhood', 'Condition 1', 'Condition 2', 'Bldg Type', 'House Style', 'Roof Style', 'Roof Matl', 'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type', 'Foundation', 'Heating', 'Garage Type', 'Misc Feature', 'Sale Type']\n",
    "df_merged_dummies = pd.get_dummies(df_merged, columns=(columns_to_getdummies), prefix=(columns_to_getdummies), prefix_sep='')\n",
    "df_merged_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting of dataset back to train and test set after addition of dummy columns for nominal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting of merged dataframe back to train and test sets\n",
    "\n",
    "df_train = df_merged_dummies.iloc[ :2050,:]\n",
    "df_test = df_merged_dummies.iloc[2051: ,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.loc[:, df_test.columns != 'SalePrice']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. SAVING 'TRAIN' AND 'TEST' DATASET DATAFRAME IN CSV FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving final dataframes in csv format\n",
    "\n",
    "df_train.to_csv('../datasets/df_train_final.csv')\n",
    "df_test.to_csv('../datasets/df_test_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. IDENTIFYING FEATURES FOR DESIGNING THE MODEL\n",
    "\n",
    "#### To identify features from the 239 columns and find out which are most correlated to 'SalePrice', two methods will be used\n",
    "\n",
    "#### 1. Filtering using correlation (Pearson's Correlation Co-efficient) to identify features with highest correlation to SalePrice\n",
    "\n",
    "#### 2. Recursive Feature Engineering (RFE) to identify optimum number of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Filtering by correlation between the features and the target variable \n",
    "\n",
    "#### Filtering is done betweeen the features and target variable 'SalePrice' and then a threshold of 0.5 is set, thus selecting features that have an absolute correlation value above 0.5. After identifying the features, it is necessary to check for any correlation between the features since one of the assumptions of Linear Regression is that all independent features are not correlated to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['SalePrice']\n",
    "X_all = df_train.loc[:, df_train.columns != 'SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filtering using correlation\n",
    "\n",
    "cor = df_train.corr()\n",
    "\n",
    "#Correlation with target variable\n",
    "cor_target = abs(cor[\"SalePrice\"])\n",
    "\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for correlation between features identifed through filtering using correlation matrix\n",
    "\n",
    "corr_features = ['Overall Qual', 'Exter Qual', 'Gr Liv Area', 'Kitchen Qual', 'Garage Area', 'Garage Cars', \n",
    "                 'Total Bsmt SF', '1st Flr SF', 'Bsmt Qual', 'Year Built', 'Garage Finish', 'Year Remod/Add', \n",
    "                 'Fireplace Qu', 'Full Bath', 'FoundationPConc', 'TotRms AbvGrd', 'Mas Vnr Area']\n",
    "\n",
    "# Checking for correlation among the features\n",
    "plt.figure(figsize=(10,10))\n",
    "mask = np.zeros_like(X_all[corr_features].corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    sns.heatmap(X_all[corr_features].corr(), mask=mask, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 17 features listed below have been identified as having an absolute correlation value above 0.5. \n",
    "\n",
    "#### features = 'Overall Qual', 'Exter Qual', 'Gr Liv Area', 'Kitchen Qual', 'Garage Area', 'Garage Cars', 'Total Bsmt SF', '1st Flr SF',         'Bsmt Qual', 'Year Built', 'Garage Finish', 'Year Remod/Add', 'Fireplace Qu', 'Full Bath', 'FoundationPConc', 'TotRms AbvGrd', 'Mas Vnr Area'.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recursive Feature Engineering (RFE) to identify optimum number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using RFE to identify optimum number of features\n",
    "\n",
    "#no of features\n",
    "nof_list=np.arange(1,20)            \n",
    "high_score=0\n",
    "\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(X_all.columns)\n",
    "model = LinearRegression()           #Initializing RFE model\n",
    "rfe = RFE(model, 13)                 #Transforming data using RFE\n",
    "X_all_rfe = rfe.fit_transform(X_all,y)       #Fitting the data to model\n",
    "model.fit(X_all_rfe,y)              \n",
    "temp = pd.Series(rfe.support_,index = cols)\n",
    "selected_features_rfe = temp[temp==True].index\n",
    "print(selected_features_rfe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at correlation between features selected by RFE - to drop features that have a high level of correlation\n",
    "\n",
    "rfe_features = ['MS SubClass60', 'MS SubClass75', 'MS SubClass120', 'MS SubClass150',\n",
    "       'Bldg TypeTwnhs', 'Roof StyleGable', 'Roof StyleGambrel',\n",
    "       'Roof StyleMansard', 'Garage Type2Types', 'Garage TypeAttchd',\n",
    "       'Garage TypeBuiltIn', 'Misc FeatureElev', 'Misc FeatureGar2']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "mask = np.zeros_like(X_all[rfe_features].corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    sns.heatmap(X_all[rfe_features].corr(), mask=mask, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The list of features from RFE are as listed:\n",
    "#### rfe_features = 'MS SubClass60', 'MS SubClass75', 'MS SubClass120', 'MS SubClass150', 'Bldg TypeTwnhs', 'Roof StyleGable', 'Roof StyleGambrel', 'Roof StyleMansard', 'Garage Type2Types', 'Garage TypeAttchd', 'Garage TypeBuiltIn', 'Misc FeatureElev', 'Misc FeatureGar2'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MODEL BUILDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: After identifying the features, it is necessary to check for any correlation between the features. One of the assumptions of Linear Regression is that all independent features are not correlated to each other. Features with correlation of more than 0.85 are dropped - 'Garage Cars'. Features identified in earlier heatmap of integer data are included in the list of selected features.These include those with high positive correlation factors seen earlier such as, Overall Qual, GR Liv Area, Garage Cars, Garage Area, Total Bsmt SF, 1st Flr SF\n",
    "#### corr_features1 = 'Overall Qual', 'Exter Qual', 'Gr Liv Area', 'Kitchen Qual', 'Garage Area', 'Garage Cars', 'Total Bsmt SF', '1st Flr SF',  'Bsmt Qual', 'Year Built', 'Garage Finish', 'Year Remod/Add', 'Fireplace Qu', 'Full Bath', 'FoundationPConc', 'TotRms AbvGrd', 'Mas Vnr Area'.  \n",
    "\n",
    "\n",
    "\n",
    "### Model 2: Another pair of features with strong correlation is 'Gr Living Area' and 'Tot Rms AbvGrd'. We will be trying another model including by dropping 'Tot Rms AbvGrd' \n",
    "#### corr_features2 = 'Overall Qual', 'Exter Qual', 'Gr Liv Area', 'Kitchen Qual', 'Garage Area', 'Garage Cars', 'Total Bsmt SF', '1st Flr SF',  'Bsmt Qual', 'Year Built', 'Garage Finish', 'Year Remod/Add', 'Fireplace Qu', 'Full Bath', 'FoundationPConc',  'Mas Vnr Area'.  \n",
    "\n",
    "\n",
    "\n",
    "### Model 3: Model designed from optimum features as identified by RFE. \n",
    "#### rfe_features = 'MS SubClass60', 'MS SubClass75', 'MS SubClass120', 'MS SubClass150', 'Bldg TypeTwnhs', 'Roof StyleGable', 'Roof StyleGambrel', 'Roof StyleMansard', 'Garage Type2Types', 'Garage TypeAttchd', 'Garage TypeBuiltIn', 'Misc FeatureElev', 'Misc FeatureGar2'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. MODEL PREPARATION \n",
    "\n",
    "#### Train-test-split:\n",
    "\n",
    "#### The train data is divided into training and testing portion to find the proper Linear Regression model. For this the model is trained on the training data and then provided the testing data. The variance between the scores predicted by the model and the actual values is measured using metrics. \n",
    "\n",
    "#### To ensure that there is a fair spread of samples between the training and testing sets, a technique called cross-validation is used. The cross validation is done by dividing the data into a required number of parts and leaving one part out as the testing data.\n",
    "\n",
    "#### For our calculations, we will be considering 30% of data to be test data for purpose of designing the model and will use a cross validation of 10 folds.\n",
    "\n",
    "#### Standard Scaler:\n",
    "#### The features are scaled to ensure they are on a compatible scale, else the difference between consecutive data points in different features may vary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiating empty dataframe to collect outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiating a dataframe to collect outcomes of evaluations\n",
    "\n",
    "outcomes ={'Model':('Mean score', 'Variance score', 'Mean Square Error', 'Root Mean Square Error')} \n",
    "df_outcomes = pd.DataFrame(outcomes)\n",
    "df_outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Model 1 - corr_features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prep\n",
    "corr_features1 = ['Overall Qual', 'Exter Qual', 'Gr Liv Area', 'Kitchen Qual', 'Garage Area', 'Garage Cars', \n",
    "                 'Total Bsmt SF', '1st Flr SF', 'Bsmt Qual', 'Year Built', 'Garage Finish', 'Year Remod/Add', \n",
    "                 'Fireplace Qu', 'Full Bath', 'FoundationPConc', 'TotRms AbvGrd', 'Mas Vnr Area']\n",
    "X= X_all[corr_features1]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size= 0.3, random_state=42)    \n",
    "\n",
    "# standard scaling\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - i. linear model (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to do linear model - for a given X_train, X_test, y_train, y_test, name of regression model\n",
    "\n",
    "def lin_reg(X_train, X_test, y_train, y_test, name):\n",
    "        \n",
    "    # instantiate linear regression model\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    # training model and getting score\n",
    "    lr_scores_train = cross_val_score(lr, X_train, y_train, cv=10)\n",
    "    print (\"Mean score:\", np.mean(lr_scores_train))\n",
    "        \n",
    "    # Fitting the model \n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # getting co-eff values\n",
    "    coeff_df = pd.DataFrame(lr.coef_ , X.columns, columns=['Coefficient'])  \n",
    "   \n",
    "    # to get predicted values\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    # Score for test data\n",
    "    print( \"Test score:\",lr.score(X_test, y_test))\n",
    "    \n",
    "    #Metrics of model\n",
    "    print (\"Means Square Error:\", metrics.mean_squared_error(y_test, pred ))\n",
    "    print(\"Root Mean Square:\", np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "    \n",
    "    perform_score=[round(np.mean(lr_scores_train),3), \n",
    "                        round(lr.score(X_test, y_test),3),\n",
    "                        round(metrics.mean_squared_error(y_test, pred ),3),\n",
    "                        round(np.sqrt(metrics.mean_squared_error(y_test, pred)),3)\n",
    "                       ]\n",
    "    # storing values in df_outcomes\n",
    "    df_outcomes[name]= pd.Series(data=perform_score)\n",
    "\n",
    "    return coeff_df, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df, pred = lin_reg(X_train, X_test, y_train, y_test, 'Linear/corr_features1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting coefficients\n",
    "sns.barplot(x =coeff_df['Coefficient'],y=coeff_df.index, data= coeff_df.sort_values(by='Coefficient') )\n",
    "\n",
    "#plotting of predictions vs Actuals\n",
    "sns.jointplot(y_test, pred)\n",
    "plt.title( 'Predictions vs Actual SalePrice, Linear Regression', loc='left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - ii. Lasso regression\n",
    "#### Calculation of optimal alpha and then fitting Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lasso regression for given X_train, X_test, y_train, y_test, name of regression model\n",
    "\n",
    "def lasso_reg(X_train, X_test, y_train, y_test, name):\n",
    "    \n",
    "    # Calculation of optimal alpha\n",
    "    optimal_lasso = LassoCV(n_alphas=500, cv=10, verbose=1)\n",
    "    optimal_lasso.fit(X_train, y_train)\n",
    "\n",
    "    # instantiate lasso model with optimal alpha\n",
    "    lasso = Lasso(alpha=optimal_lasso.alpha_)\n",
    "\n",
    "    # Training the model\n",
    "    lasso.fit(X_train, y_train)\n",
    "    lasso_scores_train = cross_val_score(lasso, X_train, y_train, cv=10)\n",
    "    print (\"Mean score:\", np.mean(lasso_scores_train))\n",
    "    \n",
    "    # getting co-eff values\n",
    "    coeff_df = pd.DataFrame(lasso.coef_ , X.columns, columns=['Coefficient'])  \n",
    "    \n",
    "    # Fitting the model\n",
    "    lasso.score(X_test, y_test)\n",
    "    print('Test score:', lasso.score(X_test, y_test))\n",
    "    pred = lasso.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    print (\"Means Square Error:\", metrics.mean_squared_error(y_test, pred ))\n",
    "    print(\"Root Mean Square:\", np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "\n",
    "    perform_score=[round(np.mean(lasso_scores_train), 4), \n",
    "                            round(lasso.score(X_test, y_test), 4), \n",
    "                            round(metrics.mean_squared_error(y_test, pred ),4), \n",
    "                            round(np.sqrt(metrics.mean_squared_error(y_test, pred)), 4)]\n",
    "\n",
    "    # storing values in df_outcomes\n",
    "    df_outcomes[name]= pd.Series(data=perform_score) \n",
    "    \n",
    "    return coeff_df, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coeff_df, pred = lasso_reg(X_train, X_test, y_train, y_test, 'Lasso/corr_features1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting coefficients\n",
    "sns.barplot(x =coeff_df['Coefficient'],y=coeff_df.index, \n",
    "            data= coeff_df.sort_values(by='Coefficient'))\n",
    "\n",
    "#plotting of predictions vs Actuals\n",
    "sns.jointplot(y_test, pred)\n",
    "plt.title( 'Predictions vs Actual SalePrice, Lasso Reg', loc='left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - iii. Ridge regression\n",
    "#### Calculation of optimal alpha and then fitting Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression for given X_train, X_test, y_train, y_test, name of regression model\n",
    "\n",
    "def ridge_reg(X_train, X_test, y_train, y_test, name):\n",
    "\n",
    "    # Calculating optimal alpha\n",
    "    ridge_alphas = np.logspace(0, 5, 200)\n",
    "\n",
    "    optimal_ridge = RidgeCV(alphas=ridge_alphas, cv=10)\n",
    "    optimal_ridge.fit(X_train, y_train)\n",
    "\n",
    "    # Training the model\n",
    "    ridge = Ridge(alpha=optimal_ridge.alpha_)\n",
    "    ridge_scores_train = cross_val_score(ridge, X_train, y_train, cv=10)\n",
    "\n",
    "    # Mean Score\n",
    "    print(\"Mean Ridge Score:\", np.mean(ridge_scores_train))\n",
    "    \n",
    "    # getting co-eff values\n",
    "    coeff_df = pd.DataFrame(ridge_coef , X.columns, columns=['Coefficient'])  \n",
    "    \n",
    "    #Fitting the model\n",
    "    ridge.fit(X_train, y_train)\n",
    "    pred = ridge.predict(X_test)\n",
    "    print(\"Test score:\",ridge.score(X_test, y_test))\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    print (\"Means Square Error:\", metrics.mean_squared_error(y_test, pred ))\n",
    "    print(\"Root Mean Square:\", np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "    \n",
    "    perform_score=[round(np.mean(ridge_scores_train),3), \n",
    "                        round(ridge.score(X_test, y_test),3),\n",
    "                        round(metrics.mean_squared_error(y_test, pred ),3),\n",
    "                        round(np.sqrt(metrics.mean_squared_error(y_test, pred)),3)\n",
    "                       ]\n",
    "    # storing values in df_outcomes\n",
    "    df_outcomes[name]= pd.Series(data=perform_score)\n",
    "\n",
    "    return coeff_df, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df, pred = ridge_reg(X_train, X_test, y_train, y_test, 'Ridge/corr_features1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting coefficients\n",
    "sns.barplot(x =coeff_df['Coefficient'],y=coeff_df.index, \n",
    "            data= coeff_df.sort_values(by='Coefficient'))\n",
    "        \n",
    "\n",
    "#plotting of predictions vs Actuals\n",
    "sns.jointplot(y_test, pred)\n",
    "plt.title( 'Predictions vs Actual SalePrice, Ridge Reg', loc='left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Model 2 - corr_features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prep\n",
    "corr_features2 = ['Overall Qual', 'Exter Qual', 'Gr Liv Area', 'Kitchen Qual', 'Garage Area', 'Garage Cars', \n",
    "                 'Total Bsmt SF', '1st Flr SF', 'Bsmt Qual', 'Year Built', 'Garage Finish', 'Year Remod/Add', \n",
    "                 'Fireplace Qu', 'Full Bath', 'FoundationPConc', 'Mas Vnr Area']\n",
    "X= X_all[corr_features2]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size= 0.3, random_state=42)    \n",
    "\n",
    "# standard scaling\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df, pred = lin_reg(X_train, X_test, y_train, y_test, 'Linear/corr_features2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting coefficients\n",
    "sns.barplot(x =coeff_df['Coefficient'],y=coeff_df.index, data= coeff_df.sort_values(by='Coefficient') )\n",
    "\n",
    "#plotting of predictions vs Actuals\n",
    "sns.jointplot(y_test, pred)\n",
    "plt.title( 'Predictions vs Actual SalePrice, Linear Regression', loc='left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df, pred = lasso_reg(X_train, X_test, y_train, y_test, 'Lasso/corr_features2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting coefficients\n",
    "sns.barplot(x =coeff_df['Coefficient'],y=coeff_df.index, \n",
    "            data= coeff_df.sort_values(by='Coefficient', ascending=False))\n",
    "        \n",
    "\n",
    "#plotting of predictions vs Actuals\n",
    "sns.jointplot(y_test, pred)\n",
    "plt.title( 'Predictions vs Actual SalePrice, Lasso Reg', loc='left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df, pred = ridge_reg(X_train, X_test, y_train, y_test, 'Ridge/corr_features2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting coefficients\n",
    "sns.barplot(x =coeff_df['Coefficient'],y=coeff_df.index, \n",
    "            data= coeff_df.sort_values(by='Coefficient'))\n",
    "        \n",
    "\n",
    "#plotting of predictions vs Actuals\n",
    "sns.jointplot(y_test, pred)\n",
    "plt.title( 'Predictions vs Actual SalePrice, Ridge Reg', loc='left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Model 3 - rfe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prep\n",
    "rfe_features = rfe_features = ['MS SubClass60', 'MS SubClass75', 'MS SubClass120', 'MS SubClass150',\n",
    "                               'Bldg TypeTwnhs', 'Roof StyleGable', 'Roof StyleGambrel',\n",
    "                                'Roof StyleMansard', 'Garage Type2Types', 'Garage TypeAttchd',\n",
    "                                'Garage TypeBuiltIn', 'Misc FeatureElev', 'Misc FeatureGar2']\n",
    "X = X_all[rfe_features]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size= 0.3, random_state=42)    \n",
    "\n",
    "# standard scaling\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df, pred = lin_reg(X_train, X_test, y_train, y_test, 'Linear/rfe_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting coefficients\n",
    "sns.barplot(x =coeff_df['Coefficient'],y=coeff_df.index, \n",
    "            data= coeff_df.sort_values(by='Coefficient'))\n",
    "        \n",
    "\n",
    "#plotting of predictions vs Actuals\n",
    "sns.jointplot(y_test, pred)\n",
    "plt.title( 'Predictions vs Actual SalePrice, Linear Reg', loc='left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df, pred = lasso_reg(X_train, X_test, y_train, y_test, 'Lasso/rfe_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting coefficients\n",
    "sns.barplot(x =coeff_df['Coefficient'],y=coeff_df.index, \n",
    "            data= coeff_df.sort_values(by='Coefficient'))\n",
    "        \n",
    "\n",
    "#plotting of predictions vs Actuals\n",
    "sns.jointplot(y_test, pred)\n",
    "plt.title( 'Predictions vs Actual SalePrice, Lasso Reg', loc='left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df, pred = ridge_reg(X_train, X_test, y_train, y_test, 'Ridge/rfe_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting coefficients\n",
    "sns.barplot(x =coeff_df['Coefficient'],y=coeff_df.index, \n",
    "            data= coeff_df.sort_values(by='Coefficient'))\n",
    "        \n",
    "\n",
    "#plotting of predictions vs Actuals\n",
    "sns.jointplot(y_test, pred)\n",
    "plt.title( 'Predictions vs Actual SalePrice, Ridge Reg', loc='left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EVALUATION OF THE MODELS\n",
    "\n",
    "### Comparing the scores and metrics of the individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the scores and metric the Ridge model with correlated features of Model 1 is preferred. This model has a high Mean(train data) score of .0.792 and has a low gap between train score (0.792) and test scores (0.855)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SUBMISSION TO KAGGLE\n",
    "\n",
    "### Ridge regression /corr_features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission based on Ridge model\n",
    "\n",
    "corr_features1 =['Overall Qual', 'Exter Qual', 'Gr Liv Area', 'Kitchen Qual', 'Garage Area', 'Garage Cars', \n",
    "                 'Total Bsmt SF', '1st Flr SF', 'Bsmt Qual', 'Year Built', 'Garage Finish', 'Year Remod/Add', \n",
    "                 'Fireplace Qu', 'Full Bath', 'FoundationPConc', 'TotRms AbvGrd', 'Mas Vnr Area']\n",
    "\n",
    "\n",
    "X = X_all[corr_features1]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "X_test_final = df_test[corr_features1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling of features\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit_transform(X)\n",
    "X = ss.transform(X)\n",
    "X_test_final = ss.transform(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of optimal alpha\n",
    "\n",
    "ridge_alphas = np.logspace(0, 5, 200)\n",
    "\n",
    "optimal_ridge = RidgeCV(alphas=ridge_alphas, cv=10)\n",
    "optimal_ridge.fit(X, y)\n",
    "\n",
    "# Training the model \n",
    "\n",
    "ridge = Ridge(alpha=optimal_ridge.alpha_)\n",
    "ridge_scores = cross_val_score(ridge, X, y, cv=10)\n",
    "print(\"Mean Ridge Score:\", np.mean(ridge_scores))\n",
    "coeff_df = pd.DataFrame(ridge.coef_, corr_features1)\n",
    "\n",
    "# Fitting the model\n",
    "ridge.fit(X, y)\n",
    "pred_final = ridge.predict(X_test_final)\n",
    "\n",
    "# Converting to prescribed format for submission to Kaggle\n",
    "df_pred_final = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': pred_final})\n",
    "df_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coeff_df = pd.DataFrame(ridge.coef_, corr_features1, columns=['Coefficient'])\n",
    "print(ridge_coeff_df)\n",
    "ridge_y_intercept = ridge.intercept_\n",
    "print(ridge_y_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion to csv file for submission to Kaggle\n",
    "df_pred_final.to_csv('../datasets/submission_ridge1rev.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission for Kaggle - Linear regression /corr_features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission based on Linear Reg model\n",
    "\n",
    "X = X_all[corr_features1]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "X_test_final = df_test[corr_features1]\n",
    "\n",
    "lr= LinearRegression()\n",
    "\n",
    "# Training scores\n",
    "lr_scores = cross_val_score(lr, X, y, cv=10)\n",
    "print (\"Mean R2:\", np.mean(lr_scores))\n",
    "\n",
    "# Fitting model and calculating predicted values\n",
    "lr.fit(X, y)\n",
    "pred_final = lr.predict(X_test_final)\n",
    "\n",
    "# Predicted values in prescribed format\n",
    "df_pred_final = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': pred_final})\n",
    "\n",
    "df_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to csv format for uploading to Kaggle\n",
    "\n",
    "df_pred_final.to_csv('../datasets/submission_lr1rev.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission for Kaggle - Lasso regression /corr_features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#submission based on Lasso model\n",
    "\n",
    "X = X_all[corr_features1]\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "X_test_final = df_test[corr_features1]\n",
    "\n",
    "# Calculation of optimal alpha\n",
    "lasso = LassoCV(n_alphas=500, cv=10, verbose=1)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Training the model\n",
    "lasso_scores = cross_val_score(lasso, X, y, cv=10)\n",
    "\n",
    "print(\"Mean Lasso Score:\", np.mean(lasso_scores))\n",
    "\n",
    "# Fitting the model\n",
    "lasso.fit(X, y)\n",
    "pred_final = lasso.predict(X_test_final)\n",
    "\n",
    "# Predicted values in format prescribed\n",
    "df_pred_final = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': pred_final})\n",
    "df_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to csv file fr uploading\n",
    "df_pred_final.to_csv('../datasets/submission_lasso1rev.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The datasets 'train.csv' and 'test.csv' were cleaned by checking for null values, converting categorical values to integers (ordinal and binary) .\n",
    "\n",
    "#### After cleaning of the datasets ('train.csv' and 'test.csv'), the various features were studied for design of the model. The heatmap was plotted to get a visual representation of the correlation between the features.\n",
    "\n",
    "#### The features for the models were shortlisted after considering the correlation of the target variable ('SalePrice') to all the features and taking a threshold of 0.5 absolute as the correlation value. Features with absolute correlation value of more that 0.5 were selected. This method is also called Filtering of features. The list of features shortlisted from correlation and heatmap is as given below.\n",
    "\n",
    "#### The 18 individual features, that were identified by the above step, were also checked for high intercorrelation to avoid multi-collinearity. Here two pairs of features were found to have high collinearity. \n",
    "    - 'Garage Cars' and 'Garage Area'  (correlation: 0.89)\n",
    "    - 'Gr Liv Area' and 'TotRms AbvGrd'(correlation: 0.81)\n",
    "        \n",
    "#### The decision to drop 'Garage Cars' for Model 1 was made since the correlation was very high. The remaining features listed above  after dropping 'Garage Cars' were considered in the design\n",
    "\n",
    "#### For Model 2 the other pair of feature with high correlation was considered and 'TotRms AbvGrd' was also dropped from the features. \n",
    "\n",
    "#### Two other variations were also tried. Polynomial to the degree of 2 gave Kaggle scores of 37656 (Public) and 35359 (Private).\n",
    "\n",
    "#### Model 3 was tried with optimal features identified through the Recursive Feature Elimination.\n",
    "\n",
    "#### After generating the model predictions, the predicted values were uploaded to Kaggle Model 1 has been identified to have the lowest Kaggle scores for Ridge and Linear. The variation in the train and test scores were also not too high and RMSE was found to be lower in the case of Linear regression model. The two models are quit simlar.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle scores for the 2 best models were as follows\n",
    "\n",
    "####  Model 1 Ridge model submission with 17 correlated features\n",
    "####  Private: 35054.17459\n",
    "#### Public: 33901.40970\n",
    "\n",
    "#### Model 1 (17 features) Linear Regression model submission \n",
    "#### Private: 35501.38125\n",
    "#### Public: 35028.56812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "                 \n",
    "#### Hence the Ridge Regression model is selected to be the preferred model to predict the target variable, based on the lower Kaggle score.\n",
    "\n",
    "#### Point to note while using the model:\n",
    "#### 1. There can be no model which is a perfect fit, however from the various models considered, this will give the least variation\n",
    "\n",
    "#### The model may be further refined by:\n",
    "    - Including other other features \n",
    "    - Feature Engineering \n",
    "    - Polynomial with degree of 3\n",
    "    - Try other known models of regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of Model 1 (17 features) Linear Regression \n",
    "\n",
    "#### Below are values for the beta coefficients and y_intercept for the individual features in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of the beta coefficients for the selected features \n",
    "\n",
    "\n",
    "print('Beta Co-efficients:')\n",
    "print(ridge_coeff_df)\n",
    "print(\"Beta_zero /y_intercept:\", ridge_y_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
